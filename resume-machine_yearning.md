**Machine Learning Yearning-Draft - Andrew Ng**

**Résumé du chapitre 1er : Why Machine Learning Strategy**

 Le Machine Learning est la base dans différentes applications : reconnaissance vocale, … Fil directeur du livre : nous créons une startup spécialisée dans la reconnaissance d’images de chats.
 
**Résumé chapitre 2eme : How to use this book to help your team** 
  
  Pour la compréhension des définitions des paramètres techniques.

**Résumé chapitre 3eme : Prerequisites and Notation** 
   
   Introduction au livre ...
   
**Résumé du Chapitre 4 : Scale drives machine learning progress**
   
    “People are now spending more time on digital devices . Their digital activities generate huge amounts of data that we can feed to our learning algorithms” if you train larger and larger neural networks, you can obtain even better performance But one of the more reliable ways to improve an algorithm’s performance today is still to train a bigger network and get more data.

![alt_text](images/Resum-Alexis0.png "image_tooltip")

![alt_text](images/Resum-Alexis1.png "image_tooltip")

![alt_text](images/Resum-Alexis2.png "image_tooltip") 

*NN = Neural Network **Résumé chapitre 5eme : Your development and test sets** Notre équipe récupère une large train set en téléchargeant des photos de chats et des photos sans chats sur différents sites. Malgré tout, la performance est insuffisante car décalage entre les photos uploadées par les utilisateurs (prises par téléphone, de moins bonne résolution ou floues) et celles récupérées sur internet pour entraîner notre training set. On a pour habitude de définir : * Un training set : sur lequel on exerce notre algorithme d’apprentissage ; * Un “dev set” (validation) : utilisé pour sélectionner les features par exemple * Un test set : pour évaluer les performances de l’algorithme. On doit choisir un dev et test set qui reflètent les données qu’on s’attend à collecter et sur lequel on souhaite réussir. **Résumé chapitre 6eme : Your dev and test sets should come from the same distribution** L’auteur conseille fortement d’utiliser les dev et test sets d’une même distribution car : * S’ils proviennent d’une distribution différente, il est plus dur de trouver les erreurs (ou ce qui fonctionne) * Cela permet de prioriser les objectifs sur lesquels travailler. **Résumé chapitre 7eme : How large do the dev/test sets need to be?** Le dev et le test set doivent être gros mais pas trop ni trop petit, suffisant pour pouvoir être évalués. **Résumé chapitre 8eme : Establish a single-number evaluation metric for your team to optimize** Classification = métrique d’évaluation à un seul nombre, contrairement au Recall ou à la Précision. Du coup, si on a vraiment besoin du Recall et de la Précision, le mieux est de prendre la moyenne des 2 (pour avoir un seul nombre) ou en combinant leur moyenne. Avoir une métrique d’évaluation à un seul nombre permet de prendre une direction claire de progrès. **Résumé chapitre 9eme : Optimizing and satisficing metrics** Autre moyen de combiner des métriques d’évaluation : comparer l’accuracy par rapport au temps de traitement : accuracy - (0.5*runningTime). Faut définir une limite à un temps de traitement acceptable et ensuite maximiser l’accuracy en respectant ces critères. Egalement, il est intéressant de minimiser le taux des FN (faux négatifs). **Résumé chapitre 10 : Having a dev set and metric speeds up iterations** Lors de la construction de la machine learning : 1. Commencer par une idée de comment construire le système 2. Implémenter l’idée dans le code 3. Expérimenter le code pour voir à quel point l'idée a fonctionné. (En général, les premières idées ne fonctionnent pas!) Sur la base de ces apprentissages, revenir en arrière pour générer plus d’idées et continuer d’itérer. C'est un processus itératif. Plus vite on boucle, plus vite on progresse. C’est pourquoi il est important de disposer de dév / test sets. Chaque fois qu’on essaie une idée, il faut mesurer les performances de l’idée sur le dév set, cela permet de décider rapidement si on se dirige dans la bonne direction. **Résumé chapitre11 : When to change dev/test sets and metrics** Le dev et test set doivent être représentatif de ce que les utilisateurs feront. Il faut aussi regarder régulièrement les résultats et voir si on garde ou change le set actuel. **Résumé chapitre 12 : Takeaways: Setting up development and test sets** Le chapitre est déjà un résumé de ce qui précède. **Résumé chapitre 13 : Build your first system quickly, then iterate** Lorsque l’on s’attaque à un domaine d’application inconnu, rien ne sert de construire un système parfait. Mieux vaut construire et entraîner un système basique rapidement (en quelques jours). Cela nous montrera une direction prometteuse dans laquelle s’investir. **Résumé chapitre 14 : Error analysis: Look at dev set examples to evaluate ideas** Le processus consistant à examiner des exemples mal classés est appelé analyse d'erreur (​**error analysis​)**. L'analyse d'erreur fait référence au processus consistant à examiner des exemples dev sets que l’algorithme a classés incorrectement, afin de comprendre les causes sous-jacentes des erreurs. Cela peut aider à hiérarchiser les projets. **Résumé chapitre 15 : Evaluating multiple ideas in parallel during error analysis** Pour corriger les erreurs, on les répartit en sous catégorie par ordre d’importance. Il faudra ensuite travailler sur la résolution de ces erreurs par ordre de priorité. **Résumé chapitre 16 : Cleaning up mislabeled dev and test set examples** Le but d’un dev set est de vous aider à rapidement évaluer si un algorithme est meilleur qu’un autre. Si une partie du dataset est mal labellisé, cela peut impacter le jugement et il est judicieux de réparer ces erreurs. **Résumé chapitre 17 : If you have a large dev set, split it into two subsets, only one of which you look at** Cela prends beaucoup de temps d’analyser un large set d’image, du coup il est plus judicieux de ne pas tous les utiliser dans une analyse d’erreur, vous pouvez utiliser une petite partie du set afin de corriger les erreurs du set. **Résumé chapitre 18 : How big should the Eyeball and Blackbox dev sets be?** L’Eyeball dev set doit être suffisamment grand pour donner une idée des principales catégories d’erreurs de l’algorithme. Si travailler sur une tâche que même les humains ne peuvent pas bien faire, alors l’exercice consistant à examiner un Eyeball dev ne sera pas aussi utile, car il est plus difficile de comprendre pourquoi l’algorithme n’a pas correctement classifié un exemple. Dans ce cas, on peut ne pas avoir un Eyeball dev set. Si on a un petit dev set, il se peut qu’on n’ait pas assez de données pour split l’Eyeball dev set et le Blackbox dev set. Ainsi, on peut utiliser l'ensemble du dev set dans l’Eyeball dev set. L’Eyeball dev set est plus important que le Blackbox set. **=> Si pas assez de données, tout mettre dans le Eyeball dev set.** **Résumé chapitre 19 : Takeaways: Basic error analysis** Pour corriger les erreurs il faut regarder qu'est ce qui ne va pas. **Résumé chapitre 20 : Bias and Variance: The two big sources of error** Il y a 2 sources principales d’erreurs : biais et variance. Les performances entre les sous-ensemble de train et validation sont en général pire que celle entre le trainfull et le test. **Résumé chapitre 21 : Examples of Bias and Variance** Overfitting = Le classificateur a très peu d'erreur d'apprentissage, mais il ne parvient pas à généraliser dans le dev set. ( % de bias bas mais % de variance haut). Underfitting = Le classificateur adapte mal l’apprentissage (ex 15 % pour le bias), mais son erreur sur le dev set (ex dev error =16 %) est à peine supérieure à l’erreur du training set. ( % de bias haut mais % de variance bas). (ex variance ici 16 %-15 % = 1%) Lorsque que le classificateur a un biais élevé et une variance élevée, il est à la fois overfitting et underfitting. Lorsque que le classificateur possède un bias bas et une variance basse alors c’est une belle performance. **Résumé chapitre 22 : Comparing to the optimal error rate** Rechercher le taux optimal d’erreur permet de mieux définir et évaluer notre modèle, en effet cela permet d’éviter certain biais ainsi que mieux planifier les étapes suivantes de création du modèle notamment sur les résultats obtenus. **Résumé chapitre 23 : Addressing Bias and Variance** Afin de réduire les problèmes de biais et la variance il y a une formule simple pour y pallier : 1. Si il y a des problèmes de biais augmentez la taille du modèle en ajoutant des couches de neurones par exemple 2. Si il y a des problèmes de variance, ajoutez plus de données à votre jeu de donnée Le fait d’ajouter des couches de neurones et ajouter des données peut pallier à la plupart des problèmes d’apprentissage du modèle. **Résumé chapitre 24 : Bias vs. Variance tradeoff** Parmi tous les changements que l’on pourrait faire sur l’algorithme, il y a ceux qui réduisent les erreurs de biais mais au prix d'accroître la variance et vice versa. Cela créé un “échange” entre biais et variance. Par exemple, augmenter la taille du modèle (en ajoutant des neurones ou couches dans un réseau de neurones ou en ajoutant des features à nos inputs) réduit en général le biais mais augmente la variance. A l’inverse, ajouter de la régularisation augmente généralement biais mais réduit la variance. **Résumé chapitre 25 : Techniques for reducing avoidable bias** * Augmenter la taille du modèle : cela va permettre de réduire le bias (et mieux entraîner le training set) mais si la variance augmente, il faut utiliser une régularisation. * Modifier les features en fonction des informations tirées de l'analyse d'erreur * Réduire ou enlever la régularisation (cela va permettre de réduire le bias mais cela augmente la variance) * Modifier l’architecture du modèle qui correspondrait mieux au problème (cela peut augmenter la variance et le bias) Une autre technique qui n’affectera pas le bias : augmenter les données dans le training set (cela va permettre de réduire des problèmes liés la variance). **Résumé chapitre 26 : Error analysis on the training set** Afin de corriger plus facilement les erreurs, il est possible de diviser les erreurs les plus importantes en sous ensemble d’erreur dans l’objectif de mieux les étudier. **Résumé chapitre 27 : Techniques for reducing variance** Si algorithme souffre d’une trop grosse variance vous devriez essayer les techniques suivante : 1. Ajouter plus de données d'entraînement 2. Ajouter “Regularisation” 3. Ajouter des “early stopping” 4. Sélectionner un certain nombre/type de features 5. Réduire la taille du modèle 6. Changer les Inputs en fonction des erreurs analysées **Résumé chapitre 28 : Diagnosing bias and variance: Learning curves** Vu que la taille du training set augmente, l’erreur du train set devrait décroître : On va souvent avoir un “taux d’erreur désiré” dont on espère que notre algorithme d’apprentissage va atteindre. Par exemple : * Si on espère une performance au niveau humain, alors le taux d’erreur humaine peut être le “taux d’erreur désiré” ; * Si notre algorithme d’apprentissage doit être utilisé pour certains buts (comme délivrer des images de chats), on doit avoir une idée du niveau de performance qui est requis pour procurer une bonne expérience utilisateur. **Résumé chapitre 29: Plotting training error** L’erreur sur le dev set diminue quand la taille du training set augmente. L’erreur sur le training set augmente quand la taille du training set augmente. Quand la taille du training set est petite, le modèle peut facilement mémoriser l’ensemble des information du training set. Quand la taille du training set est très grande, il devient difficile de mémoriser parfaitement tous les exemples du training set.


![alt_text](images/Resum-Alexis3.png "image_tooltip")

**Résumé chapitre : 30 Interpreting learning curves: High bias** Étudier les learning curves permet de mieux comprendre les biais du modèle. En effet cela permet d’éviter de rajouter inutilement des datas supplémentaire dans le modèle dans le but d’augmenter la précision. Ceci étant dû au fait que dans certains cas le nombre d’erreur dans le training set augmente proportionnellement au fur et a mesure que l’on entraine le model. **Résumé chapitre 31 : Interpreting learning curves: Other cases**


alt_text
Courbe Training error basse

Courbe Dev error est plus élevée que la Training error

Conclusion : le bias est petit mais la variance est élvée

Solution : augmenter le training data va aider à réduire le gap entre le dev error et le training error

Courbe Training error haute (bien plus haute que le niveau désiré)
Courbe Dev error bien plus élevée que la courbe Training error

Conclusion : bias et variance sont élevés

Solution : Trouver un moyen de réduire le bias et la variance dans l’algorithme

**Résumé chapitre 32 : Plotting learning curves** Si le bruit dans la courbe d’entraînement vous empêche de voir les vraies tendances, voici deux solutions: * Au lieu de former un seul modèle sur 10 exemples, choisissez plutôt plusieurs modèles (disons 3-10) différents ensembles d’entraînement choisis au hasard de 10 exemples par échantillonnage avec remplacement à partir de votre jeu initial de 100 valeurs. Entraînez un modèle différent sur chacun d’eux et calculez l’erreur sur le trainfull et sur le train de chacun des modèles résultants. Calculer et tracer l’erreur moyenne sur le trainfull et sur le train set. * Si votre jeu d’entraînement est orienté vers une classe ou s'il en a plusieurs, choisissez une option «Équilibré» au lieu de 10 exemples d’entraînement choisis au hasard parmi 100 éléments. Par exemple, vous pouvez vous assurer que 2/10 des exemples sont des exemples positifs et 8/10 des exemples négatifs. Plus généralement, assurez-vous que la fraction d’exemples de chaque classe est aussi réaliste que possible de l’ensemble global de valeurs. **Résumé chapitre : 34 How to define human-level performance**
