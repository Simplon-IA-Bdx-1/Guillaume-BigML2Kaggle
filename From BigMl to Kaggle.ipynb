{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les étapes à respecter\n",
    "\n",
    "## 1ère partie : Les Features\n",
    "\n",
    "1.1. Ouvrir les dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "fulltrain=read_csv('./cs-training.csv',index_col=0)\n",
    "    test=read_csv('./cs-test.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "index_col=0 la colonne qu'on doit utiliser comme index dans le fichier csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2. Faire le split sur le trainfull (à faire qu’une seule fois)\n",
    "\n",
    "Il faut choisir un seed pour faire un split identique à chaque fois (sinon c'est random) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = api.create_dataset(\n",
    "    origin_dataset, {\"name\": \"Dataset Name | Training\",\n",
    "                    \"sample_rate\": 0.8, \"seed\": \"my seed\"})\n",
    "test_dataset = api.create_dataset(\n",
    "    origin_dataset, {\"name\": \"Dataset Name | Test\",\n",
    "                    \"sample_rate\": 0.8, \"seed\": \"my seed\",\n",
    "                    \"out_of_bag\": True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3. Modifier les features à la fois sur le train, valid, test.\n",
    "\n",
    "Mettre les 3 dans un tableau et faire une boucle\n",
    "\n",
    "    ex :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sets = [fulltrain, train80, valid20, test]\n",
    "    for df in data_sets :\n",
    "        modifier df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4. Sauvegarder en fichier .csv les df(train80, valid20, test sets) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train80.to_csv(train80.csv,index_label='Id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2ème Partie : Créer le modèle\n",
    "\n",
    "2.1. Une fois les features modifiés, il faut envoyer les sources (csv) sur BIgML.\n",
    "\n",
    "Ceci est à faire pour les fichiers train, validation et test :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "train_src = api.create_source(train_filename) #créer les sources\n",
    "\n",
    "api.ok(train_src) # Met le programme en pause le temps que BigML finisse de télécharger\n",
    "\n",
    "train_ds = api.create_dataset(train_src) #créer les datasets\n",
    "api.ok(train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.  Maintenant qu'on a créé nos datasets, il faut créer nos modèles (à faire que sur le train car c'est le train qui nous aide à faire le modèle) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "model_args= {\"objective_field\": \"SeriousDlqin2yrs\"} #c'est là où on indique l'output field (=l'objet de la prédiction)\n",
    "\n",
    "model = api.create_ensemble(train_ds, model_args)\n",
    "api.ok(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3ème Partie : L'évaluation\n",
    "\n",
    "L'évaluation se fait sur la validation 'valid20_ds' et sur le model 'model'.\n",
    "\n",
    "Deux méthodes pour l'évaluation :\n",
    "\n",
    "* Evaluation sur BigMl :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = api.create_evaluation(model, valid_ds)\n",
    "pprint(evaluation) #pprint = pretty print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Dans cette étape, on a la matrice de confusion, l'AUC.\n",
    "\n",
    "\n",
    "* Evaluation en Python avec des fonctions à appeler :\n",
    "\n",
    "    * 1ère étape : Faire un Batch Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_prediction = api.create_batch_prediction(model, valid_ds,\n",
    "    {\n",
    "    \"name\": \"prediction.csv\",   #nom du fichier dans lequel enregistrer la prédiction\n",
    "    \"all_fields\": True,     #garder tous les champs (si on veut analyser les erreurs/features)\n",
    "    \"header\": True,     #avoir la 1ère ligne des noms de colonnes\n",
    "    \"confidence\": True,     #nous donne l'indice de confiance entre 0,5 et 1\n",
    "    \"probabilities\": True       #colonnes de '0 proba' et '1 proba' mais ce qui nous intéresse c'est la colonne '1 proba'\n",
    "    }                      )\n",
    "api.ok(batch_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ouvrir le csv et le transformer pour avoir un dataframe avec la fonction read_csv (prediction_df).\n",
    "\n",
    "* \n",
    "    * 2ème étape : Calcul de la matrice de confusion avec pandas_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_ml import ConfusionMatrix\n",
    "\n",
    "confusion_matrix = ConfusionMatrix(prediction['SeriousDlqin2yrs'],prediction['SeriousDlqin2yrs.1'])\n",
    "\n",
    "d = {'P\\u0302': [confusion_matrix.TP, confusion_matrix.FP, confusion_matrix.PPV],\n",
    "     'N\\u0302': [confusion_matrix.FN, confusion_matrix.TN, confusion_matrix.NPV],\n",
    "     'Recall': [confusion_matrix.TPR, confusion_matrix.TNR, confusion_matrix.ACC]}\n",
    "confusion_matrix_df = DataFrame(data=d,index=['P','N', 'Precision'])\n",
    "confusion_matrix_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* \n",
    "    * 3ème étape : L'analyse de gain/coût\n",
    "\n",
    "Attention pseudo-code:\n",
    "\n",
    "```\n",
    "Trier le dataframe selon la colonne '1 proba'\n",
    "Calculer les seuils intermédiaires : \n",
    "    Copier la colonne '1 proba' en '1 proba b'\n",
    "    Décaler la colonne '1 proba b' de 1 vers le bas (fonction shift(fill_value=0))\n",
    "    Créer colonne seuil qui est la moyenne de '1 proba' et '1 proba b'\n",
    "Pour chaque seuil :\n",
    "    Calculer le nombre d'élément dans chaque classe : TP, TN, FP, FN\n",
    "    Calculer le coût en multipliant le nombre d'élements par leur coût correspondant\n",
    "    Enregistrer le résultat dans une colonne coût #ici on a calculé le gain pour chaque seuil intermédiaire\n",
    "\n",
    "On cherche l'index qui correspond au meilleur gain (idxmax) :\n",
    "\n",
    "max_index = cost_df['cost'].idxmax() #index qui correspond au gain maximum\n",
    "max_gain = cost_df.loc[max_index]['cost'] # gain maximum\n",
    "max_threshold = cost_df.loc[max_index]['threshold'] #seuil optimal\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * 4ème étape :\n",
    "   * Calcul à la main de l'AUC\n",
    "\n",
    "Attention pseudo-code:\n",
    "```\n",
    "score=0\n",
    "Pour chaque positif P:\n",
    "    Pour chaque négatif N:\n",
    "        Si '1 proba' de P > '1 proba' de N:\n",
    "            score+=1\n",
    "AUC = score/(nombre de N * nombre de P)\n",
    "```\n",
    "```\n",
    "nombre de TP = 0\n",
    "score = 0\n",
    "Trier selon '1 proba' décroissant\n",
    "\n",
    "Pour chaque ligne:\n",
    "    Si la ligne est positive:\n",
    "        nombre de TP +=1\n",
    "    Sinon:\n",
    "        score += nombre de TP\n",
    "AUC = score/(nombre de N * nombre de P)\n",
    "```\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * Calcul de l'AUC avec sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "AUC = score/(nombre de N * nombre de P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4ème Partie : Analyse d'erreurs\n",
    "\n",
    "4.1. Trier les prédictions par erreur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "prediction_df['absolute_error']=(prediction_df['1 probability']-prediction_df['SeriousDlqin2yrs']).abs()\n",
    "# différence entre la prédiction et la vraie valeur (en valeur absolue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "prediction_errors = prediction_df.sort_values(by='absolute_error', ascending=False).head(100)\n",
    "#on trie par le niveau d'erreur et on sélectionne les 100 plus grosses erreurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2. On sépare ensuite les FP et FN.\n",
    "\n",
    "4.3. On analyse les erreurs en essayant de comprendre pourquoi on s'est trompé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5ème Partie : Identifier les axes d'amélioration et reprendre à l'étape 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6ème Partie : Soumettre à Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On créé un dataset en copiant la colonne index car sur Kaggle il faut que les numéros correspondent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "kaggle_prediction_df=DataFrame(index=prediction_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On copie la colonne 1 probability qu'on renomme Probability pour Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "kaggle_prediction_df['Probability']=prediction_df['1 probability']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On définit un nom de fichier csv qu'on envoie à Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "kaggle_prediction_file=\"kaggleprediction.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On enregistre tout le dataframe dans le fichier qu'on a définit. On utilise la colonne index comme colonne Id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "kaggle_prediction_df.to_csv(kaggle_prediction_file,index_label='Id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On envoie la prédiction sur Kaggle. (1er arg = nom_du_fichier, 2e arg= commentaire ou nom de la soumission, 3e arg = nom du challenge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "kaggle.api.competition_submit(kaggle_prediction_file, \"commentaire sur ma prediction\", \"GiveMeSomeCredit\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
